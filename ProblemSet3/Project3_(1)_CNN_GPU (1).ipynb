{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project3 (1)_CNN_GPU.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tbfz3zSXBt3H"
      },
      "source": [
        "# <p style=\"text-align: center;\">MIS 284N - Big Data and Distributed Programming</p>\n",
        "## <p style=\"text-align: center;\">Project 3 - Machine Learning using Tensorflow and Google Colab</p>\n",
        "## <p style=\"text-align: center;\">Total points: 100</p>\n",
        "## <p style=\"text-align: center;\">Due: Sunday, October 17th submitted via Canvas by 11:59 pm</p>\n",
        "\n",
        "This will be a in-class project done in teams of 2. \n",
        "\n",
        "In this Project, we will work with CIFAR10 image dataset. \n",
        "The starter code to download the database using keras is given below. \n",
        "Test the project on Google Colab running on a CPU, GPU and TPU\n",
        " \n",
        "\n",
        "# In every line of code, please write a comment to briefly explain what that line is doing.\n",
        "Your grades will be based on your understanding of the code you write! \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnHTzAuvxxQT"
      },
      "source": [
        "# Task 1\n",
        "Convert the features in a form that can be given as input to tensorflow library/functions\n",
        "\n",
        "In this task you will perform data augmentation. That is, pre-process the data to make the model more robust. Experiment with data augmentation techniques like rotation, translation, horizontal-flip, scaling, ZCA whitening and histogram equalization. \n",
        "You can choose any two or more augmentation technique(s) of your choice. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfXRqFgivcVv"
      },
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from keras.callbacks import ModelCheckpoint\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9qrDetJJofS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b95e47b0-1286-45d2-ca62-ee60e6c80d20"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBd8Lsmevt4G",
        "outputId": "1f0150fe-bc39-4255-fac9-c579c93652c9"
      },
      "source": [
        "x_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGtIK4Rxvwsd",
        "outputId": "c05e3605-2ab6-4367-ecea-37d4e68ff124"
      },
      "source": [
        "y_test.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkCCXOk7vnbF",
        "outputId": "c099bbe5-6805-4d44-8c51-4df1cd33709f"
      },
      "source": [
        "x_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K1UpFxU4vrjh",
        "outputId": "4fb6aafc-e3da-4fb7-91bc-0b41d8d1efb0"
      },
      "source": [
        "y_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50000, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6PnpWlOv41n"
      },
      "source": [
        "# Normalize data.\n",
        "x_train = x_train.astype('float32') / 255\n",
        "x_test = x_test.astype('float32') / 255\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKxKdyLlwzoq"
      },
      "source": [
        "## Data Augmentation \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDS-Mu3_v8he",
        "outputId": "cc05fd2f-7d4e-419b-e827-ed349be9a888"
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6],\n",
              "       [9],\n",
              "       [9],\n",
              "       ...,\n",
              "       [9],\n",
              "       [1],\n",
              "       [1]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6c8xzloyoUD"
      },
      "source": [
        "# Task 2\n",
        "Try to build a Neural Network model, train on the features and report the accuracy.\n",
        "Report your observations on the time taken on CPU and GPU (with and without CuDNN kernel) \n",
        "\n",
        "\n",
        "\n",
        "1.   Create a CNN based model with 4 hidden layers with 64, 128, 256 and 512 units in each succesive layer. Use a 5x5 convolution kernel and change as necessary. (Use at least 2 augmentations on your input) \n",
        "2.   Create an LSTM based model with 1 LSTM layer with 256 units. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kT7mmmvIwUAC"
      },
      "source": [
        "##CNN Model\n",
        "\n",
        "def create_model():\n",
        "  model = tf.keras.models.Sequential()\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
        "  model.add(tf.keras.layers.Conv2D(64, (5, 5), padding='same', activation='elu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
        "  model.add(tf.keras.layers.Conv2D(128, (5, 5), padding='same', activation='elu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
        "  model.add(tf.keras.layers.Conv2D(256, (5, 5), padding='same', activation='elu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "  model.add(tf.keras.layers.BatchNormalization(input_shape=x_train.shape[1:]))\n",
        "  model.add(tf.keras.layers.Conv2D(512, (5, 5), padding='same', activation='elu'))\n",
        "  model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "  model.add(tf.keras.layers.Dropout(0.25))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(512))\n",
        "  model.add(tf.keras.layers.Activation('elu'))\n",
        "  model.add(tf.keras.layers.Dropout(0.5))\n",
        "  model.add(tf.keras.layers.Dense(10))\n",
        "  model.add(tf.keras.layers.Activation('softmax'))\n",
        "  return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztKAq7U_wcu6"
      },
      "source": [
        "with tf.device('/device:GPU:0'):\n",
        "  model = create_model()\n",
        "  model.compile(\n",
        "      \n",
        "      optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "      loss='sparse_categorical_crossentropy',\n",
        "      metrics=['sparse_categorical_accuracy'])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wg2-y7OhyEZN",
        "outputId": "758d7d7c-a4ae-4a2a-f613-492a81888cff"
      },
      "source": [
        "%%time\n",
        "\n",
        "print('Using real-time data augmentation.')\n",
        "print(\"CNN with GPU\")\n",
        "# Training parameters\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 100\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "filepath = 'cifar10.h5'\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "'''checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "callbacks = [checkpoint]'''\n",
        "with tf.device('/device:GPU:0'):\n",
        "  # This will do preprocessing and realtime data augmentation:\n",
        "  datagen = ImageDataGenerator(\n",
        "      \n",
        "      # set input mean to 0 over the dataset\n",
        "      featurewise_center=False,\n",
        "      # set each sample mean to 0\n",
        "      samplewise_center=False,\n",
        "      # divide inputs by std of dataset\n",
        "      featurewise_std_normalization=False,\n",
        "      # divide each input by its std\n",
        "      samplewise_std_normalization=False,\n",
        "      # apply ZCA whitening\n",
        "      zca_whitening=True,\n",
        "      # epsilon for ZCA whitening\n",
        "      zca_epsilon=1e-06,\n",
        "      # randomly rotate images in the range (deg 0 to 180)\n",
        "      rotation_range=180,\n",
        "      # randomly shift images horizontally\n",
        "      width_shift_range=0.1,\n",
        "      # randomly shift images vertically\n",
        "      height_shift_range=0.1,\n",
        "      # set range for random shear\n",
        "      shear_range=0.2,\n",
        "      # set range for random zoom\n",
        "      zoom_range=0.2,\n",
        "      # set range for random channel shifts\n",
        "      #channel_shift_range=0.,\n",
        "      # set mode for filling points outside the input boundaries\n",
        "      fill_mode='nearest',\n",
        "      # value used for fill_mode = \"constant\"\n",
        "      cval=0.,\n",
        "      # randomly flip images\n",
        "      horizontal_flip=True,\n",
        "      # randomly flip images\n",
        "      vertical_flip=False,\n",
        "      # set rescaling factor (applied before any other transformation)\n",
        "      rescale=None,\n",
        "      # set function that will be applied on each input\n",
        "      preprocessing_function=None,\n",
        "      # image data format, either \"channels_first\" or \"channels_last\"\n",
        "      data_format=None,\n",
        "      # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "      validation_split=0.0)\n",
        "\n",
        "  # Compute quantities required for featurewise normalization\n",
        "  # (std, mean, and principal components if ZCA whitening is applied).\n",
        "\n",
        "  datagen.fit(x_train)\n",
        "\n",
        "  \n",
        "  import time\n",
        "  print(\"CNN GPU\")\n",
        "  start = time.time()\n",
        "\n",
        "  # Fit the model on the batches generated by datagen.flow().\n",
        "  model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                      validation_data=(x_test, y_test),\n",
        "                      epochs=epochs, verbose=1, workers=4\n",
        "                      )\n",
        "  stop = time.time()\n",
        "  print(f\"Training time: {stop - start}s\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using real-time data augmentation.\n",
            "CNN with GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:337: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN GPU\n",
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1563/1563 [==============================] - 210s 134ms/step - loss: 1.2940 - sparse_categorical_accuracy: 0.5456 - val_loss: 5.7156 - val_sparse_categorical_accuracy: 0.1000\n",
            "Epoch 2/100\n",
            "1563/1563 [==============================] - 217s 139ms/step - loss: 1.2681 - sparse_categorical_accuracy: 0.5560 - val_loss: 3.5852 - val_sparse_categorical_accuracy: 0.0810\n",
            "Epoch 3/100\n",
            "1563/1563 [==============================] - 221s 141ms/step - loss: 1.2454 - sparse_categorical_accuracy: 0.5656 - val_loss: 6.2714 - val_sparse_categorical_accuracy: 0.1006\n",
            "Epoch 4/100\n",
            "1563/1563 [==============================] - 215s 137ms/step - loss: 1.2363 - sparse_categorical_accuracy: 0.5704 - val_loss: 4.2314 - val_sparse_categorical_accuracy: 0.1002\n",
            "Epoch 5/100\n",
            "1563/1563 [==============================] - 217s 138ms/step - loss: 1.2210 - sparse_categorical_accuracy: 0.5746 - val_loss: 4.3091 - val_sparse_categorical_accuracy: 0.0987\n",
            "Epoch 6/100\n",
            "1563/1563 [==============================] - 218s 139ms/step - loss: 1.2110 - sparse_categorical_accuracy: 0.5793 - val_loss: 4.1459 - val_sparse_categorical_accuracy: 0.0999\n",
            "Epoch 7/100\n",
            "1563/1563 [==============================] - 213s 136ms/step - loss: 1.2012 - sparse_categorical_accuracy: 0.5819 - val_loss: 3.7095 - val_sparse_categorical_accuracy: 0.1148\n",
            "Epoch 8/100\n",
            "1563/1563 [==============================] - 209s 134ms/step - loss: 1.1954 - sparse_categorical_accuracy: 0.5862 - val_loss: 2.9791 - val_sparse_categorical_accuracy: 0.0760\n",
            "Epoch 9/100\n",
            "1563/1563 [==============================] - 207s 132ms/step - loss: 1.1852 - sparse_categorical_accuracy: 0.5907 - val_loss: 4.5259 - val_sparse_categorical_accuracy: 0.0856\n",
            "Epoch 10/100\n",
            "1563/1563 [==============================] - 200s 128ms/step - loss: 1.1793 - sparse_categorical_accuracy: 0.5935 - val_loss: 3.4420 - val_sparse_categorical_accuracy: 0.0979\n",
            "Epoch 11/100\n",
            "1563/1563 [==============================] - 197s 126ms/step - loss: 1.1775 - sparse_categorical_accuracy: 0.5918 - val_loss: 3.9292 - val_sparse_categorical_accuracy: 0.0716\n",
            "Epoch 12/100\n",
            "1563/1563 [==============================] - 194s 124ms/step - loss: 1.1759 - sparse_categorical_accuracy: 0.5927 - val_loss: 3.5280 - val_sparse_categorical_accuracy: 0.0994\n",
            "Epoch 13/100\n",
            "1563/1563 [==============================] - 196s 125ms/step - loss: 1.1585 - sparse_categorical_accuracy: 0.6018 - val_loss: 4.4933 - val_sparse_categorical_accuracy: 0.0900\n",
            "Epoch 14/100\n",
            "1563/1563 [==============================] - 196s 125ms/step - loss: 1.1562 - sparse_categorical_accuracy: 0.6006 - val_loss: 4.3058 - val_sparse_categorical_accuracy: 0.0714\n",
            "Epoch 15/100\n",
            "1563/1563 [==============================] - 198s 127ms/step - loss: 1.1503 - sparse_categorical_accuracy: 0.6045 - val_loss: 5.1782 - val_sparse_categorical_accuracy: 0.1000\n",
            "Epoch 16/100\n",
            "1563/1563 [==============================] - 194s 124ms/step - loss: 1.1436 - sparse_categorical_accuracy: 0.6051 - val_loss: 2.8731 - val_sparse_categorical_accuracy: 0.0652\n",
            "Epoch 17/100\n",
            "1563/1563 [==============================] - 194s 124ms/step - loss: 1.1434 - sparse_categorical_accuracy: 0.6072 - val_loss: 5.1922 - val_sparse_categorical_accuracy: 0.1005\n",
            "Epoch 18/100\n",
            "1563/1563 [==============================] - 193s 123ms/step - loss: 1.1409 - sparse_categorical_accuracy: 0.6079 - val_loss: 4.2500 - val_sparse_categorical_accuracy: 0.1000\n",
            "Epoch 19/100\n",
            "1563/1563 [==============================] - 196s 125ms/step - loss: 1.1340 - sparse_categorical_accuracy: 0.6110 - val_loss: 2.8963 - val_sparse_categorical_accuracy: 0.0998\n",
            "Epoch 20/100\n",
            "1563/1563 [==============================] - 192s 123ms/step - loss: 1.1353 - sparse_categorical_accuracy: 0.6114 - val_loss: 4.0769 - val_sparse_categorical_accuracy: 0.0962\n",
            "Epoch 21/100\n",
            "1563/1563 [==============================] - 208s 133ms/step - loss: 1.1250 - sparse_categorical_accuracy: 0.6150 - val_loss: 4.5582 - val_sparse_categorical_accuracy: 0.1000\n",
            "Epoch 22/100\n",
            "1563/1563 [==============================] - 205s 131ms/step - loss: 1.1253 - sparse_categorical_accuracy: 0.6153 - val_loss: 3.8693 - val_sparse_categorical_accuracy: 0.0849\n",
            "Epoch 23/100\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 1.1288 - sparse_categorical_accuracy: 0.6131 - val_loss: 3.4076 - val_sparse_categorical_accuracy: 0.1013\n",
            "Epoch 24/100\n",
            "1563/1563 [==============================] - 215s 138ms/step - loss: 1.1291 - sparse_categorical_accuracy: 0.6147 - val_loss: 2.9087 - val_sparse_categorical_accuracy: 0.1052\n",
            "Epoch 25/100\n",
            "1563/1563 [==============================] - 225s 144ms/step - loss: 1.1226 - sparse_categorical_accuracy: 0.6132 - val_loss: 3.2965 - val_sparse_categorical_accuracy: 0.0966\n",
            "Epoch 26/100\n",
            "1563/1563 [==============================] - 218s 139ms/step - loss: 1.1124 - sparse_categorical_accuracy: 0.6196 - val_loss: 3.5537 - val_sparse_categorical_accuracy: 0.0734\n",
            "Epoch 27/100\n",
            "1563/1563 [==============================] - 217s 139ms/step - loss: 1.1181 - sparse_categorical_accuracy: 0.6182 - val_loss: 2.9896 - val_sparse_categorical_accuracy: 0.1042\n",
            "Epoch 28/100\n",
            "1563/1563 [==============================] - 215s 137ms/step - loss: 1.1119 - sparse_categorical_accuracy: 0.6189 - val_loss: 2.8674 - val_sparse_categorical_accuracy: 0.0879\n",
            "Epoch 29/100\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 1.1159 - sparse_categorical_accuracy: 0.6197 - val_loss: 2.6770 - val_sparse_categorical_accuracy: 0.1074\n",
            "Epoch 30/100\n",
            "1563/1563 [==============================] - 221s 141ms/step - loss: 1.1163 - sparse_categorical_accuracy: 0.6191 - val_loss: 2.8200 - val_sparse_categorical_accuracy: 0.1000\n",
            "Epoch 31/100\n",
            "1563/1563 [==============================] - 242s 154ms/step - loss: 1.1093 - sparse_categorical_accuracy: 0.6224 - val_loss: 2.7921 - val_sparse_categorical_accuracy: 0.0740\n",
            "Epoch 32/100\n",
            "1563/1563 [==============================] - 219s 140ms/step - loss: 1.0997 - sparse_categorical_accuracy: 0.6243 - val_loss: 2.6839 - val_sparse_categorical_accuracy: 0.1043\n",
            "Epoch 33/100\n",
            "1563/1563 [==============================] - 232s 148ms/step - loss: 1.1038 - sparse_categorical_accuracy: 0.6229 - val_loss: 3.3800 - val_sparse_categorical_accuracy: 0.0849\n",
            "Epoch 34/100\n",
            "1563/1563 [==============================] - 213s 136ms/step - loss: 1.0989 - sparse_categorical_accuracy: 0.6246 - val_loss: 3.1239 - val_sparse_categorical_accuracy: 0.1001\n",
            "Epoch 35/100\n",
            "1563/1563 [==============================] - 218s 139ms/step - loss: 1.1089 - sparse_categorical_accuracy: 0.6211 - val_loss: 3.7172 - val_sparse_categorical_accuracy: 0.1000\n",
            "Epoch 36/100\n",
            "1563/1563 [==============================] - 211s 135ms/step - loss: 1.1051 - sparse_categorical_accuracy: 0.6242 - val_loss: 3.4403 - val_sparse_categorical_accuracy: 0.0993\n",
            "Epoch 37/100\n",
            "1563/1563 [==============================] - 216s 138ms/step - loss: 1.0927 - sparse_categorical_accuracy: 0.6301 - val_loss: 4.9444 - val_sparse_categorical_accuracy: 0.1000\n",
            "Epoch 38/100\n",
            " 206/1563 [==>...........................] - ETA: 3:03 - loss: 1.0828 - sparse_categorical_accuracy: 0.6368"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dAVUVIA9LQI"
      },
      "source": [
        "CNN On CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drzdzbI0-q-8"
      },
      "source": [
        "model_cpu = create_model()\n",
        "model_cpu.compile(\n",
        "    \n",
        "      \n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, ),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['sparse_categorical_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oqm48sLs9OPa",
        "outputId": "f2b0782b-4c33-492e-c1b7-d2d666194b8c"
      },
      "source": [
        "%%time\n",
        "print('Using real-time data augmentation.')\n",
        "print(\"CNN ON CPU\")\n",
        "# Training parameters\n",
        "batch_size = 32  # orig paper trained all networks with batch_size=128\n",
        "epochs = 10\n",
        "data_augmentation = True\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "# Prepare model model saving directory.\n",
        "filepath = 'cifar10.h5'\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             monitor='val_acc',\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "callbacks = [checkpoint]\n",
        "# This will do preprocessing and realtime data augmentation:\n",
        "datagen = ImageDataGenerator(\n",
        "    \n",
        "      \n",
        "    # set input mean to 0 over the dataset\n",
        "    featurewise_center=False,\n",
        "    # set each sample mean to 0\n",
        "    samplewise_center=False,\n",
        "    # divide inputs by std of dataset\n",
        "    featurewise_std_normalization=False,\n",
        "    # divide each input by its std\n",
        "    samplewise_std_normalization=False,\n",
        "    # apply ZCA whitening\n",
        "    zca_whitening=True,\n",
        "    # epsilon for ZCA whitening\n",
        "    zca_epsilon=1e-06,\n",
        "    # randomly rotate images in the range (deg 0 to 180)\n",
        "    rotation_range=180,\n",
        "    # randomly shift images horizontally\n",
        "    width_shift_range=0.1,\n",
        "    # randomly shift images vertically\n",
        "    height_shift_range=0.1,\n",
        "    # set range for random shear\n",
        "    shear_range=0.2,\n",
        "    # set range for random zoom\n",
        "    zoom_range=0.2,\n",
        "    # set range for random channel shifts\n",
        "    #channel_shift_range=0.,\n",
        "    # set mode for filling points outside the input boundaries\n",
        "    fill_mode='nearest',\n",
        "    # value used for fill_mode = \"constant\"\n",
        "    cval=0.,\n",
        "    # randomly flip images\n",
        "    horizontal_flip=True,\n",
        "    # randomly flip images\n",
        "    vertical_flip=False,\n",
        "    # set rescaling factor (applied before any other transformation)\n",
        "    rescale=None,\n",
        "    # set function that will be applied on each input\n",
        "    preprocessing_function=None,\n",
        "    # image data format, either \"channels_first\" or \"channels_last\"\n",
        "    data_format=None,\n",
        "    # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "    validation_split=0.0)\n",
        "\n",
        "# Compute quantities required for featurewise normalization\n",
        "# (std, mean, and principal components if ZCA whitening is applied).\n",
        "\n",
        "datagen.fit(x_train)\n",
        "import time\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "print(\"CNN on CPU\")\n",
        "\n",
        "# Fit the model on the batches generated by datagen.flow().\n",
        "model_cpu.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                    validation_data=(x_test, y_test),\n",
        "                    epochs=epochs, verbose=1, workers=4,\n",
        "                    callbacks=callbacks)\n",
        "stop = time.time()\n",
        "print(f\"Training time CPU: {stop - start}s\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using real-time data augmentation.\n",
            "CNN ON CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras_preprocessing/image/image_data_generator.py:337: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
            "  warnings.warn('This ImageDataGenerator specifies '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CNN on CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            " 628/1563 [===========>..................] - ETA: 15:55 - loss: 2.7716 - sparse_categorical_accuracy: 0.1718"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RR5sLHcY58Bs"
      },
      "source": [
        "### Build a new model with CuDNN kernel\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mvl8qtBYxNbn"
      },
      "source": [
        "# Score trained model.\n",
        "scores = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', scores[0])\n",
        "print('Test accuracy:', scores[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UigmhHzAwtSU"
      },
      "source": [
        "model.save_weights('./cifar.h5', overwrite=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOMqFeFxqV_j"
      },
      "source": [
        "# Task 3\n",
        "Run the LSTM solution in Task2 on a TPU and report the performance "
      ]
    }
  ]
}